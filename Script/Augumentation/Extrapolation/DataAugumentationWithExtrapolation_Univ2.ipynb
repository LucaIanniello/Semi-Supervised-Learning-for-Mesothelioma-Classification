{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import h5py"
      ],
      "metadata": {
        "id": "9HPBozsXB85r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://zenodo.org/records/15732622/files/datasetTrident_univ2.zip?download=1\"\n",
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Train.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = './'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('./Train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNrSVq29p3r",
        "outputId": "6e9bb906-a176-4564-b81a-dadae1270eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-26 15:06:03--  https://zenodo.org/records/15732622/files/datasetTrident_univ2.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1469096082 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘/content/Train.zip’\n",
            "\n",
            "/content/Train.zip  100%[===================>]   1.37G  21.1MB/s    in 74s     \n",
            "\n",
            "2025-06-26 15:07:18 (18.9 MB/s) - ‘/content/Train.zip’ saved [1469096082/1469096082]\n",
            "\n",
            "Extracted files: ['.config', 'trident_processed_univ2', 'Train.zip', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNWLQCNk485g"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from typing import Tuple, List, Union\n",
        "\n",
        "class WSIFeatureExtrapolation:\n",
        "    \"\"\"\n",
        "    Implementazione dell'extrapolazione per feature estratte da WSI.\n",
        "\n",
        "    Uso:\n",
        "        extrapolator = WSIFeatureExtrapolation()\n",
        "        augmented_features, indices = extrapolator.generate_extrapolated_features(\n",
        "            your_wsi_features,\n",
        "            lambda_values=[0.3, 0.5],\n",
        "            n_augmentations_per_patch=2\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors: int = 8):\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def extrapolate_patch(self, c_j: np.ndarray, c_k: np.ndarray, lambda_param: float) -> np.ndarray:\n",
        "        \"\"\"Formula: c'_j = (c_j - c_k) * λ + c_j\"\"\"\n",
        "        return (c_j - c_k) * lambda_param + c_j\n",
        "\n",
        "    def generate_extrapolated_features(self,\n",
        "                                     features: np.ndarray,\n",
        "                                     lambda_values: Union[float, List[float]] = 0.5,\n",
        "                                     n_augmentations_per_patch: int = 2) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Genera feature extrapolate.\n",
        "\n",
        "        Args:\n",
        "            features: Array (n_patches, n_features) delle feature originali\n",
        "            lambda_values: Valori del parametro λ per l'extrapolazione\n",
        "            n_augmentations_per_patch: Numero di augmentazioni per patch\n",
        "\n",
        "        Returns:\n",
        "            (extrapolated_features, original_indices)\n",
        "        \"\"\"\n",
        "        if isinstance(lambda_values, (int, float)):\n",
        "            lambda_values = [lambda_values]\n",
        "\n",
        "        n_patches, n_features = features.shape\n",
        "\n",
        "        # Setup nearest neighbors\n",
        "        nn_model = NearestNeighbors(n_neighbors=min(self.n_neighbors, n_patches))\n",
        "        nn_model.fit(features)\n",
        "\n",
        "        # Calcola dimensioni output\n",
        "        total_augmentations = n_patches * n_augmentations_per_patch * len(lambda_values)\n",
        "\n",
        "        # Pre-alloca arrays per efficienza\n",
        "        extrapolated = np.zeros((total_augmentations, n_features), dtype=np.float32)\n",
        "        original_indices = np.zeros(total_augmentations, dtype=np.int32)\n",
        "\n",
        "        # Trova neighbors per tutte le patch\n",
        "        distances, neighbor_indices = nn_model.kneighbors(features)\n",
        "\n",
        "        augmentation_idx = 0\n",
        "\n",
        "        for patch_idx in range(n_patches):\n",
        "            c_j = features[patch_idx]\n",
        "\n",
        "            # Neighbors escludendo se stesso\n",
        "            neighbors = neighbor_indices[patch_idx]\n",
        "            if neighbors[0] == patch_idx:\n",
        "                available_neighbors = neighbors[1:]\n",
        "            else:\n",
        "                available_neighbors = neighbors\n",
        "\n",
        "            for aug_count in range(n_augmentations_per_patch):\n",
        "                if len(available_neighbors) > 0:\n",
        "                    neighbor_idx = np.random.choice(available_neighbors)\n",
        "                    c_k = features[neighbor_idx]\n",
        "\n",
        "                    for lambda_val in lambda_values:\n",
        "                        extrapolated[augmentation_idx] = self.extrapolate_patch(c_j, c_k, lambda_val)\n",
        "                        original_indices[augmentation_idx] = patch_idx\n",
        "                        augmentation_idx += 1\n",
        "\n",
        "        return extrapolated[:augmentation_idx], original_indices[:augmentation_idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extrapolator = WSIFeatureExtrapolation()"
      ],
      "metadata": {
        "id": "9rWbPXoU9YiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESEMPIO D'USO per il tuo caso specifico:\n",
        "file_h5 = \"./trident_processed_univ2/B/20x_256px_0px_overlap/features_uni_v2/M-104.h5\"\n",
        "\n",
        "with h5py.File(file_h5, 'r') as f:\n",
        "    features = torch.from_numpy(f['features'][:]).float()\n",
        "    num_patch = features.shape[0]  # features è il tuo array numpy o h5py\n",
        "    labels = np.zeros(num_patch, dtype=np.int64)  # oppure np.int32\n",
        "    labels = torch.from_numpy(labels)\n",
        "\n",
        "wsi_features = features  # shape: (n_patches, 1024)\n",
        "\n",
        "# # Applica extrapolazione\n",
        "extrapolated_features, indices = extrapolator.generate_extrapolated_features(\n",
        "     wsi_features,\n",
        "     lambda_values=[0.3],  # Valori conservativi\n",
        "     n_augmentations_per_patch=1\n",
        ")\n",
        "#\n",
        "# # Combina originali e augmentate per il training\n",
        "extrapolated_labels = torch.from_numpy(np.zeros(extrapolated_features.shape[0], dtype=np.int64))\n"
      ],
      "metadata": {
        "id": "aHcsw-PPBzZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2a2a6e-e364-49c7-a985-9dd84eb168ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-2479020122.py:77: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
            "  extrapolated[augmentation_idx] = self.extrapolate_patch(c_j, c_k, lambda_val)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasetUNIV2Extrapolation\n"
      ],
      "metadata": {
        "id": "pdSb0zN3fh9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_h5 = \"./trident_processed_univ2/B/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "folder_dest = \"./datasetUNIV2Extrapolation/\"\n",
        "extrapolator = WSIFeatureExtrapolation()\n",
        "B = []\n",
        "\n",
        "results = {}\n",
        "\n",
        "for fname in os.listdir(folder_h5):\n",
        "    if not fname.endswith('.h5'):\n",
        "        continue\n",
        "    file_h5 = os.path.join(folder_h5, fname)\n",
        "    with h5py.File(file_h5, 'r') as f:\n",
        "        features_np = f['features'][:]\n",
        "    features = torch.from_numpy(features_np).float()\n",
        "    num_patch = features.shape[0]\n",
        "    labels_np = np.zeros(num_patch, dtype=np.int64)\n",
        "    labels = torch.from_numpy(labels_np)\n",
        "\n",
        "    extrapolated_features_np, indices = extrapolator.generate_extrapolated_features(\n",
        "        features_np,\n",
        "        lambda_values=[0.3],\n",
        "        n_augmentations_per_patch=1\n",
        "    )\n",
        "\n",
        "    extrapolated_labels_np = np.zeros(extrapolated_features_np.shape[0], dtype=np.int64)\n",
        "    extrapolated_labels = torch.from_numpy(extrapolated_labels_np)\n",
        "    extrapolated_features = torch.from_numpy(extrapolated_features_np).float()\n",
        "\n",
        "    fname = fname.split('.')[0]\n",
        "    torch.save(features, folder_dest + fname + \".pt\")\n",
        "    torch.save(extrapolated_features, folder_dest + fname + \"_ext.pt\")\n",
        "    B.append(fname)\n",
        "    B.append(fname + \"_ext\")\n"
      ],
      "metadata": {
        "id": "iiX7hTUBJD87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_h5 = \"./trident_processed_univ2/E/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "folder_dest = \"./datasetUNIV2Extrapolation/\"\n",
        "extrapolator = WSIFeatureExtrapolation()\n",
        "\n",
        "E = []\n",
        "\n",
        "results = {}\n",
        "\n",
        "for fname in os.listdir(folder_h5):\n",
        "    if not fname.endswith('.h5'):\n",
        "        continue\n",
        "    file_h5 = os.path.join(folder_h5, fname)\n",
        "    with h5py.File(file_h5, 'r') as f:\n",
        "        features_np = f['features'][:]\n",
        "    features = torch.from_numpy(features_np).float()\n",
        "    num_patch = features.shape[0]\n",
        "    labels_np = np.zeros(num_patch, dtype=np.int64)\n",
        "    labels = torch.from_numpy(labels_np)\n",
        "\n",
        "    extrapolated_features_np, indices = extrapolator.generate_extrapolated_features(\n",
        "        features_np,\n",
        "        lambda_values=[0.3],\n",
        "        n_augmentations_per_patch=1\n",
        "    )\n",
        "\n",
        "    extrapolated_labels_np = np.zeros(extrapolated_features_np.shape[0], dtype=np.int64)\n",
        "    extrapolated_labels = torch.from_numpy(extrapolated_labels_np)\n",
        "    extrapolated_features = torch.from_numpy(extrapolated_features_np).float()\n",
        "\n",
        "    fname = fname.split('.')[0]\n",
        "    torch.save(features, folder_dest + fname + \".pt\")\n",
        "    torch.save(extrapolated_features, folder_dest + fname + \"_ext.pt\")\n",
        "\n",
        "    E.append(fname)\n",
        "    E.append(fname + \"_ext\")\n"
      ],
      "metadata": {
        "id": "C1UwGnaXNUfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_h5 = \"./trident_processed_univ2/S/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "folder_dest = \"./datasetUNIV2Extrapolation/\"\n",
        "extrapolator = WSIFeatureExtrapolation()\n",
        "S = []\n",
        "results = {}\n",
        "\n",
        "for fname in os.listdir(folder_h5):\n",
        "    if not fname.endswith('.h5'):\n",
        "        continue\n",
        "    file_h5 = os.path.join(folder_h5, fname)\n",
        "    with h5py.File(file_h5, 'r') as f:\n",
        "        features_np = f['features'][:]\n",
        "    features = torch.from_numpy(features_np).float()\n",
        "    num_patch = features.shape[0]\n",
        "    labels_np = np.zeros(num_patch, dtype=np.int64)\n",
        "    labels = torch.from_numpy(labels_np)\n",
        "\n",
        "    extrapolated_features_np, indices = extrapolator.generate_extrapolated_features(\n",
        "        features_np,\n",
        "        lambda_values=[0.3],\n",
        "        n_augmentations_per_patch=1\n",
        "    )\n",
        "\n",
        "    extrapolated_labels_np = np.zeros(extrapolated_features_np.shape[0], dtype=np.int64)\n",
        "    extrapolated_labels = torch.from_numpy(extrapolated_labels_np)\n",
        "    extrapolated_features = torch.from_numpy(extrapolated_features_np).float()\n",
        "\n",
        "    fname = fname.split('.')[0]\n",
        "    torch.save(features, folder_dest + fname + \".pt\")\n",
        "    torch.save(extrapolated_features, folder_dest + fname + \"_ext_01.pt\")\n",
        "\n",
        "\n",
        "    extrapolated_features_np, indices = extrapolator.generate_extrapolated_features(\n",
        "        features_np,\n",
        "        lambda_values=[0.5],\n",
        "        n_augmentations_per_patch=1\n",
        "    )\n",
        "\n",
        "    extrapolated_labels_np = np.zeros(extrapolated_features_np.shape[0], dtype=np.int64)\n",
        "    extrapolated_labels = torch.from_numpy(extrapolated_labels_np)\n",
        "    extrapolated_features = torch.from_numpy(extrapolated_features_np).float()\n",
        "\n",
        "    torch.save(extrapolated_features, folder_dest + fname + \"_ext_02.pt\")\n",
        "    S.append(fname)\n",
        "    S.append(fname + \"_ext_01\")\n",
        "    S.append(fname + \"_ext_02\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LR_26nxrNXdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(B)"
      ],
      "metadata": {
        "id": "3VbtET_EhZeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d8f2c5-d2d9-4351-8996-c023855b0ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['M-30', 'M-30_ext', 'M-32', 'M-32_ext', 'M-108', 'M-108_ext', 'M-112', 'M-112_ext', 'M-121', 'M-121_ext', 'M-105', 'M-105_ext', 'M-24', 'M-24_ext', 'M-104', 'M-104_ext']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_csv = []\n",
        "for i in range(len(B)):\n",
        "    v = B[i].split('.')[0]\n",
        "    dataset_csv.append([v,v,'B'])\n",
        "for i in range(len(S)):\n",
        "    v = S[i].split('.')[0]\n",
        "    dataset_csv.append([v,v,'S'])\n",
        "for i in range(len(E)):\n",
        "    v = E[i].split('.')[0]\n",
        "    dataset_csv.append([v,v,'E'])"
      ],
      "metadata": {
        "id": "6LcjTAFaOdtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Nome del file CSV\n",
        "file_csv = folder_dest + 'datasetComposition.csv'\n",
        "\n",
        "# Scrittura del file CSV\n",
        "with open(file_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Scrivere l'intestazione\n",
        "    writer.writerow(['case_id', 'slide_id', 'label'])\n",
        "    # Scrivere i dati\n",
        "    writer.writerows(dataset_csv)\n",
        "\n",
        "print(f\"File CSV salvato: {file_csv}\")"
      ],
      "metadata": {
        "id": "46078aXHOTUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1052db-d6e2-47be-a4ba-be5bcfec1911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File CSV salvato: ./datasetUNIV2Extrapolation/datasetComposition.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un archivio ZIP della cartella\n",
        "def create_zip_archive(folder_path, zip_name=None):\n",
        "    \"\"\"\n",
        "    Crea un archivio ZIP di una cartella mantenendo la struttura\n",
        "    \"\"\"\n",
        "    if zip_name is None:\n",
        "        zip_name = f\"{os.path.basename(folder_path)}.zip\"\n",
        "\n",
        "    print(f\"🗜️ Creando archivio ZIP: {zip_name}\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Mantiene la struttura delle cartelle nell'archivio\n",
        "                arc_name = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
        "                zipf.write(file_path, arc_name)\n",
        "                print(f\"  📁 Aggiunto: {arc_name}\")\n",
        "\n",
        "    # Mostra dimensione dell'archivio\n",
        "    zip_size = os.path.getsize(zip_name)\n",
        "    print(f\"✅ Archivio creato: {zip_name} ({zip_size/1024/1024:.2f} MB)\")\n",
        "    return zip_name"
      ],
      "metadata": {
        "id": "Bc9iDR5Aenvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso della cartella da caricare\n",
        "zip_filename = 'datasetCompleted.zip'  # Nome dell'archivio\n",
        "zip_path = create_zip_archive(folder_dest, zip_filename)\n"
      ],
      "metadata": {
        "id": "1GniSF3lerUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60379e3e-a333-483a-b7c4-554c8d18cd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗜️ Creando archivio ZIP: datasetCompleted.zip\n",
            "  📁 Aggiunto: M-86_ext_01.pt\n",
            "  📁 Aggiunto: M-65_ext_01.pt\n",
            "  📁 Aggiunto: M-11.pt\n",
            "  📁 Aggiunto: M-109.pt\n",
            "  📁 Aggiunto: M-87_ext_01.pt\n",
            "  📁 Aggiunto: M-103_ext.pt\n",
            "  📁 Aggiunto: M-108.pt\n",
            "  📁 Aggiunto: M-10_ext.pt\n",
            "  📁 Aggiunto: M-105_ext.pt\n",
            "  📁 Aggiunto: M-110_ext.pt\n",
            "  📁 Aggiunto: M-101_ext_02.pt\n",
            "  📁 Aggiunto: M-30.pt\n",
            "  📁 Aggiunto: datasetComposition.csv\n",
            "  📁 Aggiunto: M-87_ext_02.pt\n",
            "  📁 Aggiunto: M-113.pt\n",
            "  📁 Aggiunto: M-114_ext_01.pt\n",
            "  📁 Aggiunto: M-104_ext.pt\n",
            "  📁 Aggiunto: M-114.pt\n",
            "  📁 Aggiunto: M-32_ext.pt\n",
            "  📁 Aggiunto: M-110.pt\n",
            "  📁 Aggiunto: M-100_ext.pt\n",
            "  📁 Aggiunto: M-112_ext.pt\n",
            "  📁 Aggiunto: M-10.pt\n",
            "  📁 Aggiunto: M-101_ext_01.pt\n",
            "  📁 Aggiunto: M-121.pt\n",
            "  📁 Aggiunto: M-87.pt\n",
            "  📁 Aggiunto: M-24.pt\n",
            "  📁 Aggiunto: M-108_ext.pt\n",
            "  📁 Aggiunto: M-103.pt\n",
            "  📁 Aggiunto: M-109_ext.pt\n",
            "  📁 Aggiunto: M-86.pt\n",
            "  📁 Aggiunto: M-114_ext_02.pt\n",
            "  📁 Aggiunto: M-32.pt\n",
            "  📁 Aggiunto: M-121_ext.pt\n",
            "  📁 Aggiunto: M-24_ext.pt\n",
            "  📁 Aggiunto: M-105.pt\n",
            "  📁 Aggiunto: M-104.pt\n",
            "  📁 Aggiunto: M-30_ext.pt\n",
            "  📁 Aggiunto: M-100.pt\n",
            "  📁 Aggiunto: M-111_ext.pt\n",
            "  📁 Aggiunto: M-112.pt\n",
            "  📁 Aggiunto: M-101.pt\n",
            "  📁 Aggiunto: M-86_ext_02.pt\n",
            "  📁 Aggiunto: M-65.pt\n",
            "  📁 Aggiunto: M-11_ext.pt\n",
            "  📁 Aggiunto: M-111.pt\n",
            "  📁 Aggiunto: M-113_ext.pt\n",
            "  📁 Aggiunto: M-65_ext_02.pt\n",
            "✅ Archivio creato: datasetCompleted.zip (3379.64 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Token di accesso Zenodo (sostituisci con il tuo)\n",
        "ACCESS_TOKEN = 'uVSb7icJqT9efPM71KYgviJ50r7eML9ynei2q7hDkedVlFrf8fBsr9lFaJ3O'\n",
        "\n",
        "# Crea una nuova deposizione\n",
        "def create_deposition(title):\n",
        "    url = 'https://zenodo.org/api/deposit/depositions'\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "\n",
        "    data = {\n",
        "        'metadata': {\n",
        "            'title': title,\n",
        "            'upload_type': 'dataset',\n",
        "            'description': 'Dataset WSI project MLiA',\n",
        "            'creators': [{'name': 'Raf-Tony-Luca'}]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    r = requests.post(url, params=params, data=json.dumps(data), headers=headers)\n",
        "    return r.json()\n",
        "\n",
        "# Carica il file\n",
        "def upload_file(deposition_id, file_path):\n",
        "    # Get bucket URL\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.get(url, params=params)\n",
        "    bucket_url = r.json()[\"links\"][\"bucket\"]\n",
        "\n",
        "    # Upload file\n",
        "    filename = os.path.basename(file_path)\n",
        "    with open(file_path, \"rb\") as fp:\n",
        "        r = requests.put(f\"{bucket_url}/{filename}\",\n",
        "                        data=fp,\n",
        "                        params=params)\n",
        "    return r.json()\n",
        "\n",
        "# Pubblica il dataset\n",
        "def publish_deposition(deposition_id):\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}/actions/publish'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.post(url, params=params)\n",
        "    return r.json()\n"
      ],
      "metadata": {
        "id": "m-Y4EomGe3Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esegui l'upload\n",
        "print(\"Creando deposizione...\")\n",
        "deposition = create_deposition(\"dataset_trident_univ2_extrapolation\")\n",
        "deposition_id = deposition['id']\n",
        "\n",
        "print(f\"Caricando file... (ID: {deposition_id})\")\n",
        "upload_result = upload_file(deposition_id, zip_filename)\n",
        "\n",
        "print(\"Pubblicando dataset...\")\n",
        "publication = publish_deposition(deposition_id)\n",
        "\n",
        "print(f\"Dataset pubblicato! DOI: {publication['doi']}\")\n",
        "print(f\"URL: {publication['links']['record_html']}\")"
      ],
      "metadata": {
        "id": "z_Z2pS6Ue5-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6f4609-6f03-41e3-9a50-8e6d14bd66ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando deposizione...\n",
            "Caricando file... (ID: 15747608)\n",
            "Pubblicando dataset...\n",
            "Dataset pubblicato! DOI: 10.5281/zenodo.15747608\n",
            "URL: https://zenodo.org/record/15747608\n"
          ]
        }
      ]
    }
  ]
}