{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = \"resnet50\"\n",
        "#feature_extractor = \"phykon\"\n",
        "#feature_extractor = \"univ1\"\n",
        "#feature_extractor = \"univ2\"\n",
        "IN_DIM_M = 1024\n",
        "N_epoch = 30\n",
        "N_epoch_dif = 30\n",
        "N_epoch_aug = 30"
      ],
      "metadata": {
        "id": "Txrkeqex4z38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "if feature_extractor == \"resnet50\":\n",
        "  url = \"https://zenodo.org/records/15711642/files/datasetTrident_resnet.zip?download=1\"\n",
        "  rootB = \"./trident_processed_resnet/B/20x_256px_0px_overlap/features_resnet50/\"\n",
        "  rootE = \"./trident_processed_resnet/E/20x_256px_0px_overlap/features_resnet50/\"\n",
        "  rootS = \"./trident_processed_resnet/S/20x_256px_0px_overlap/features_resnet50/\"\n",
        "elif feature_extractor == \"phykon\":\n",
        "  url = \"https://zenodo.org/records/15736995/files/datasetTrident_phikon.zip?download=1\"\n",
        "  rootB = \"./trident_processed_phikon/B/20x_224px_0px_overlap/features_phikon_v2/\"\n",
        "  rootE = \"./trident_processed_phikon/E/20x_224px_0px_overlap/features_phikon_v2/\"\n",
        "  rootS = \"./trident_processed_phikon/S/20x_224px_0px_overlap/features_phikon_v2/\"\n",
        "elif feature_extractor == \"univ1\":\n",
        "  url = \"https://zenodo.org/records/15711374/files/datasetTrident_univ1.zip?download=1\"\n",
        "  rootB = \"./trident_processed_univ1/B/20x_256px_0px_overlap/features_uni_v1/\"\n",
        "  rootE = \"./trident_processed_univ1/E/20x_256px_0px_overlap/features_uni_v1/\"\n",
        "  rootS = \"./trident_processed_univ1/S/20x_256px_0px_overlap/features_uni_v1/\"\n",
        "elif feature_extractor == \"univ2\":\n",
        "  url = \"https://zenodo.org/records/15732622/files/datasetTrident_univ2.zip?download=1\"\n",
        "  rootB = \"./trident_processed_univ2/B/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "  rootE = \"./trident_processed_univ2/E/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "  rootS = \"./trident_processed_univ2/S/20x_256px_0px_overlap/features_uni_v2/\"\n",
        "  IN_DIM_M = 1536\n",
        "else:\n",
        "  raise ValueError(\"Feature extractor not supported\")\n"
      ],
      "metadata": {
        "id": "ScfjSWQBN0Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Train.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = './'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('./Train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "del url, extract_path, extracted_files"
      ],
      "metadata": {
        "id": "GTNrSVq29p3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import h5py"
      ],
      "metadata": {
        "id": "9HPBozsXB85r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install denoising-diffusion-pytorch"
      ],
      "metadata": {
        "id": "GVgbmlWnLBdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.py\n",
        "import os, h5py, torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WSIFeatureBag(Dataset):\n",
        "    def __init__(self, h5_path, cls_label):\n",
        "        self.path = h5_path\n",
        "        self.label = torch.tensor(cls_label, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):          # un bag = una slide\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.path, 'r') as f:\n",
        "            feats = torch.from_numpy(f['features'][:]).float()  # (N,1024)\n",
        "        return feats, self.label\n"
      ],
      "metadata": {
        "id": "4XujdcPFyDTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models/vae.py\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class FeatureVAE(nn.Module):\n",
        "    def __init__(self, in_dim=1024, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(in_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.mu = nn.Linear(256, latent_dim)\n",
        "        self.logv = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, 512)\n",
        "        self.fc5 = nn.Linear(512, in_dim)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
        "        return self.mu(h), self.logv(h)\n",
        "\n",
        "    def reparameterize(self, mu, logv):\n",
        "        std = torch.exp(0.5*logv)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(F.relu(self.fc3(z))))\n",
        "        return self.fc5(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logv = self.encode(x)\n",
        "        z  = self.reparameterize(mu, logv)\n",
        "        xr = self.decode(z)\n",
        "        return xr, mu, logv\n"
      ],
      "metadata": {
        "id": "QqkskDJeyfy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build_dataset.py\n",
        "from glob import glob\n",
        "from torch.utils.data import ConcatDataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "filesB   = sorted(glob(f\"{rootB}/*.h5\"))\n",
        "labelsB = np.zeros(len(filesB), dtype=int)\n",
        "filesE   = sorted(glob(f\"{rootE}/*.h5\"))\n",
        "labelsE = np.ones(len(filesE), dtype=int)\n",
        "filesS   = sorted(glob(f\"{rootS}/*.h5\"))\n",
        "labelsS = np.ones(len(filesS), dtype=int)*2\n",
        "\n",
        "# Flatten the list of files and concatenate the labels\n",
        "files = filesB + filesE + filesS\n",
        "labels = np.concatenate([labelsB, labelsE, labelsS])\n",
        "\n",
        "\n",
        "bags = [WSIFeatureBag(p,l) for p,l in zip(files, labels)]\n",
        "dataset = ConcatDataset(bags)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "del bags, files, labels"
      ],
      "metadata": {
        "id": "yZ5knes8yP2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = FeatureVAE(in_dim = IN_DIM_M).cuda()\n",
        "opt = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
        "for epoch in range(N_epoch):\n",
        "    for feats,_ in loader:\n",
        "        feats = feats.cuda()\n",
        "        xr, mu, logv = vae(feats)\n",
        "        mse = F.mse_loss(xr, feats)\n",
        "        kld = -0.5*torch.mean(1+logv-mu.pow(2)-logv.exp())\n",
        "        loss = mse + 1e-3*kld\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        del feats, xr, mu, logv\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch} --- MSE: {mse * 1000:.4f}, KLD: {kld * 1000:.4f}, Loss: {loss * 1000:.4f}\")\n",
        "print(\"Done!\")\n",
        "print(f\"Epoch {N_epoch} MSE: {mse * 1000:.4f}, KLD: {kld * 1000:.4f}, Loss: {loss * 1000:.4f}\")"
      ],
      "metadata": {
        "id": "vlKDejG06nAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
        "\n",
        "def create_diffusion_model(height=8, width=16):\n",
        "    unet = Unet(\n",
        "        dim=64,\n",
        "        dim_mults=(1, 2, 4),\n",
        "        channels=1\n",
        "    )\n",
        "\n",
        "    diffusion = GaussianDiffusion(\n",
        "        model=unet,\n",
        "        image_size=(height, width),  # Dimensioni corrette\n",
        "        timesteps=250,\n",
        "        sampling_timesteps=100\n",
        "    )\n",
        "    return diffusion\n"
      ],
      "metadata": {
        "id": "z6zdhU9e7c-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def train_diffusion_on_latents(vae, bag_loader, device='cuda', epochs=10):\n",
        "    \"\"\"\n",
        "    Funzione corretta per allenare il diffusion model su tutte le patch di tutte le WSIs\n",
        "    senza confondere i livelli di DataLoader\n",
        "    \"\"\"\n",
        "    # 1. Estrai tutti i latenti da tutte le WSIs\n",
        "    all_latents = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for bag, _ in bag_loader:  # bag_loader restituisce un bag per iterazione\n",
        "            feats = bag.squeeze(0).to(device)  # [N_patches, 1024]\n",
        "\n",
        "            # Processa in chunk per evitare OOM\n",
        "            chunk_size = 256\n",
        "            z_chunks = []\n",
        "            for i in range(0, feats.size(0), chunk_size):\n",
        "                chunk = feats[i:i+chunk_size]\n",
        "                z_mu, _ = vae.encode(chunk)\n",
        "                z_reshaped = z_mu.view(-1, 1, 8, 16)\n",
        "                z_chunks.append(z_reshaped.cpu())\n",
        "\n",
        "            all_latents.append(torch.cat(z_chunks, dim=0))\n",
        "\n",
        "    # 2. Combina tutti i latenti in un unico dataset\n",
        "    latent_dataset = torch.cat(all_latents, dim=0)  # [N_tot_patch, 1, 8, 16]\n",
        "\n",
        "    # 3. Crea DataLoader per i patch latenti\n",
        "    patch_loader = DataLoader(\n",
        "        TensorDataset(latent_dataset),\n",
        "        batch_size=128,\n",
        "        shuffle=True,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # 4. Inizializza modello diffusion\n",
        "    diffusion = create_diffusion_model().to(device)\n",
        "    optimizer = torch.optim.Adam(diffusion.parameters(), lr=8e-5)\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Per mixed precision\n",
        "\n",
        "    # 5. Allenamento\n",
        "    diffusion.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i, (latents,) in enumerate(patch_loader):\n",
        "            latents = latents.to(device, non_blocking=True)\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = diffusion(latents)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                avg_loss = total_loss / (i + 1)\n",
        "                print(f\"Epoca {epoch+1} | Batch {i}/{len(patch_loader)} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        avg_epoch_loss = total_loss / len(patch_loader)\n",
        "        print(f\"Epoca {epoch+1} completata | Loss media: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    return diffusion\n"
      ],
      "metadata": {
        "id": "4XYNRGfvSAL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train_diffusion_on_latents_old(vae, data_loader, device='cuda', epochs=50):\n",
        "    vae.eval()\n",
        "\n",
        "    # Calcola le dimensioni di reshape\n",
        "    latent_dim = vae.latent_dim\n",
        "    height = 8  # Fattore di 128\n",
        "    width = 16   # 8 * 16 = 128\n",
        "\n",
        "    # Estrai e rimodella i latenti\n",
        "    all_latents = []\n",
        "    with torch.no_grad():\n",
        "        for feats, _ in data_loader:\n",
        "            feats = feats.squeeze(0).to(device)\n",
        "            z_mu, _ = vae.encode(feats)\n",
        "            z_reshaped = z_mu.view(-1, 1, height, width)\n",
        "            all_latents.append(z_reshaped.cpu())\n",
        "            del feats, z_mu, z_reshaped\n",
        "\n",
        "    # Training del diffusion model\n",
        "    del data_loader\n",
        "    print(f\"Reshaped latents shape: {all_latents[0].shape}\")\n",
        "    print(f\"Total number of latents: {len(all_latents)}\")\n",
        "\n",
        "    print(\"Training Diffusion Model...\")\n",
        "    latent_dataset = torch.cat(all_latents, dim=0)\n",
        "    latent_loader = DataLoader(TensorDataset(latent_dataset), batch_size=128, shuffle=True)\n",
        "\n",
        "    diffusion = create_diffusion_model(height, width).to(device)\n",
        "    optimizer = torch.optim.Adam(diffusion.parameters(), lr=8e-5)\n",
        "\n",
        "    diffusion.train()\n",
        "    for epoch in range(epochs):\n",
        "        i = 0\n",
        "        for (latents,) in latent_loader:\n",
        "            latents = latents.to(device)\n",
        "            loss = diffusion(latents)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            i += 1\n",
        "            if i % 100 == 0:\n",
        "              print(f\" Batch {i}/{len(latent_loader)}\")\n",
        "            del latents\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    return diffusion\n"
      ],
      "metadata": {
        "id": "5yT4-JPa7htg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusion = train_diffusion_on_latents(vae, loader, epochs=N_epoch_dif)"
      ],
      "metadata": {
        "id": "mBYdz_gj3DIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_features_kstep(feats, vae, diffusion_model, K=0.2):\n",
        "    device = feats.device\n",
        "    with torch.no_grad():\n",
        "        # Encode tutta la WSI\n",
        "        z_mu, _ = vae.encode(feats)  # [N_patches, 128]\n",
        "        z_input = z_mu.view(-1, 1, 8, 16)  # [N_patches, 1, 8, 16]\n",
        "\n",
        "        # K-step diffusion\n",
        "        T = diffusion_model.num_timesteps\n",
        "        k = int(K * T)\n",
        "        t = torch.full((z_input.shape[0],), k, device=device, dtype=torch.long)\n",
        "        z_noisy = diffusion_model.q_sample(z_input, t)\n",
        "\n",
        "        # Reverse diffusion\n",
        "        z_denoised = diffusion_model.p_sample_loop(\n",
        "            shape=z_noisy.shape,\n",
        "            return_all_timesteps=False\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        z_final = z_denoised.view(-1, 128)\n",
        "        augmented_feats = vae.decode(z_final)\n",
        "\n",
        "    return torch.cat([augmented_feats], dim=0)\n"
      ],
      "metadata": {
        "id": "owMRBvBlSUxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_features_kstep_old(original_feats, vae, diffusion_model, K=0.2):\n",
        "    vae.eval()\n",
        "    diffusion_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_mu, _ = vae.encode(original_feats)\n",
        "        z_input = z_mu.view(-1, 1, 8, 16)  # Stessa dimensione del training\n",
        "\n",
        "        # K-step diffusion\n",
        "        T = diffusion_model.num_timesteps\n",
        "        k = int(K * T)\n",
        "        t = torch.full((z_input.shape[0],), k, device=original_feats.device, dtype=torch.long)\n",
        "        z_noisy = diffusion_model.q_sample(z_input, t)\n",
        "\n",
        "        # Reverse diffusion\n",
        "        z_denoised = diffusion_model.p_sample_loop(\n",
        "            shape=z_noisy.shape,\n",
        "            return_all_timesteps=False\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        z_final = z_denoised.view(-1, 128)\n",
        "        augmented_feats = vae.decode(z_final)\n",
        "\n",
        "    return torch.cat([augmented_feats], dim=0)\n"
      ],
      "metadata": {
        "id": "t_kzrDoi2XQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class ABMIL(nn.Module):\n",
        "    def __init__(self, in_dim=1024, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.Tanh()\n",
        "        )\n",
        "        self.att  = nn.Sequential(\n",
        "            nn.Linear(256,128), nn.Tanh(),\n",
        "            nn.Linear(128,1)\n",
        "        )\n",
        "        self.cls  = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, bag):\n",
        "        H = self.embed(bag)              # (n_inst,256)\n",
        "        A = torch.softmax(self.att(H),0) # (n_inst,1)\n",
        "        M = torch.sum(A*H, 0, keepdim=True)\n",
        "        return self.cls(M)               # (1,3)\n",
        "\n",
        "def train_mil_with_augdiff(vae, diffusion_model, data_loader, device='cuda',epochs = 4):\n",
        "    mil_model = ABMIL(in_dim=1024, n_classes=3).to(device)\n",
        "    optimizer = torch.optim.Adam(mil_model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for bag, label in data_loader:\n",
        "            original_feats = bag.squeeze(0).to(device)  # [n_patches, 1024]\n",
        "\n",
        "            # ‚úì FEATURE AUGMENTATION con AugDiff\n",
        "            augmented_feats = augment_features_kstep(\n",
        "                original_feats, vae, diffusion_model, K=0.2\n",
        "            )\n",
        "\n",
        "            # MIL forward pass\n",
        "            logits = mil_model(augmented_feats)\n",
        "            loss = F.cross_entropy(logits, label.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del bag, label, original_feats, augmented_feats, logits\n",
        "\n",
        "    return mil_model\n",
        "device = 'cuda'\n",
        "mil_model = train_mil_with_augdiff(vae, diffusion, loader,device,N_epoch_aug)\n",
        "'''"
      ],
      "metadata": {
        "id": "TGroDuPv7qnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasetResnetExtrapolation"
      ],
      "metadata": {
        "id": "2ka9J_1O_Um8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "NNXy54vZFAib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_features_kstep_batched(feats, vae, diffusion, K=0.2, batch_size=256):\n",
        "    augmented = []\n",
        "    for i in range(0, feats.shape[0], batch_size):\n",
        "        chunk = feats[i:i+batch_size]\n",
        "        chunk = chunk.cuda()\n",
        "        aug_chunk = augment_features_kstep(chunk, vae, diffusion, K=K)\n",
        "        augmented.append(aug_chunk.cpu())\n",
        "        del chunk, aug_chunk\n",
        "        torch.cuda.empty_cache()\n",
        "    return torch.cat(augmented, dim=0)\n"
      ],
      "metadata": {
        "id": "Y2vOAn5_Qk1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_f = []\n",
        "E_f = []\n",
        "S_f = []\n",
        "\n",
        "bag_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Controlla la shape del primo elemento del dataset\n",
        "first_item = dataset[0]\n",
        "if isinstance(first_item, tuple):\n",
        "    features, label = first_item\n",
        "    print(f\"Shape delle features nel dataset: {features.shape}\")\n",
        "    print(f\"Shape della label nel dataset: {label.shape if hasattr(label, 'shape') else type(label)}\")\n",
        "else:\n",
        "    print(f\"Shape del primo elemento del dataset: {first_item.shape}\")\n",
        "\n",
        "# Controlla la shape del primo batch del DataLoader\n",
        "for batch in bag_loader:\n",
        "    if isinstance(batch, tuple):\n",
        "        batch_features, batch_label = batch\n",
        "        print(f\"Shape delle features nel primo batch: {batch_features.shape}\")\n",
        "        print(f\"Shape delle label nel primo batch: {batch_label.shape if hasattr(batch_label, 'shape') else type(batch_label)}\")\n",
        "    else:\n",
        "        # When batch_size is 1, batch is a list [data, label]\n",
        "        batch_features = batch[0]\n",
        "        batch_label = batch[1]\n",
        "        print(f\"Shape delle features nel primo batch: {batch_features.shape}\")\n",
        "        print(f\"Shape delle label nel primo batch: {batch_label.shape if hasattr(batch_label, 'shape') else type(batch_label)}\")\n",
        "    break  # Solo il primo batch\n",
        "    del batch_features, batch_label\n",
        "del first_item"
      ],
      "metadata": {
        "id": "Qi-8u5JN-t7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bag, y in bag_loader:\n",
        "    feats = bag[0]  # (N,1024)\n",
        "    aug_feats = augment_features_kstep_batched(feats, vae, diffusion, K=0.2, batch_size=256)\n",
        "    print(f\"Shape delle features nel primo batch: {feats.shape}\")\n",
        "    print(f\"Shape delle features generate nel primo batch: {aug_feats.shape}\")\n",
        "\n",
        "    if y == 0:\n",
        "        B_f.append(feats)\n",
        "        B_f.append(aug_feats)\n",
        "    elif y == 1:\n",
        "        E_f.append(feats)\n",
        "        E_f.append(aug_feats)\n",
        "    else:\n",
        "        S_f.append(feats)\n",
        "        S_f.append(aug_feats)\n"
      ],
      "metadata": {
        "id": "u1VRDcFVI1DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = filesS\n",
        "\n",
        "labels = np.concatenate([labelsS])\n",
        "\n",
        "bags = [WSIFeatureBag(p,l) for p,l in zip(files, labels)]\n",
        "dataset = ConcatDataset(bags)\n",
        "del bags, files, labels\n",
        "bag_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "for bag, y in bag_loader:\n",
        "    feats = bag[0]  # (N,1024)\n",
        "    aug_feats = augment_features_kstep_batched(feats, vae, diffusion, K=0.2, batch_size=256)\n",
        "    if y == 0:\n",
        "        B_f.append(feats)\n",
        "        B_f.append(aug_feats)\n",
        "    elif y == 1:\n",
        "        E_f.append(feats)\n",
        "        E_f.append(aug_feats)\n",
        "    else:\n",
        "        S_f.append(feats)\n",
        "        S_f.append(aug_feats)"
      ],
      "metadata": {
        "id": "uQyxtfumBjY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_csv = []\n",
        "j = 0\n",
        "\n",
        "folder_dest = \"./datasetResnetExtrapolation/\"\n",
        "\n",
        "for i in range(len(B_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(B_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'B'])\n",
        "for i in range(len(S_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(S_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'S'])\n",
        "for i in range(len(E_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(E_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'E'])"
      ],
      "metadata": {
        "id": "44Jd-KzxB7La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Nome del file CSV\n",
        "file_csv = folder_dest + 'datasetComposition.csv'\n",
        "\n",
        "# Scrittura del file CSV\n",
        "with open(file_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Scrivere l'intestazione\n",
        "    writer.writerow(['case_id', 'slide_id', 'label'])\n",
        "    # Scrivere i dati\n",
        "    writer.writerows(dataset_csv)\n",
        "\n",
        "print(f\"File CSV salvato: {file_csv}\")"
      ],
      "metadata": {
        "id": "Tl8P8JkqCyAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un archivio ZIP della cartella\n",
        "def create_zip_archive(folder_path, zip_name=None):\n",
        "    \"\"\"\n",
        "    Crea un archivio ZIP di una cartella mantenendo la struttura\n",
        "    \"\"\"\n",
        "    if zip_name is None:\n",
        "        zip_name = f\"{os.path.basename(folder_path)}.zip\"\n",
        "\n",
        "    print(f\"üóúÔ∏è Creando archivio ZIP: {zip_name}\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Mantiene la struttura delle cartelle nell'archivio\n",
        "                arc_name = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
        "                zipf.write(file_path, arc_name)\n",
        "                print(f\"  üìÅ Aggiunto: {arc_name}\")\n",
        "\n",
        "    # Mostra dimensione dell'archivio\n",
        "    zip_size = os.path.getsize(zip_name)\n",
        "    print(f\"‚úÖ Archivio creato: {zip_name} ({zip_size/1024/1024:.2f} MB)\")\n",
        "    return zip_name"
      ],
      "metadata": {
        "id": "zZhX2Y68C54e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso della cartella da caricare\n",
        "zip_filename = 'datasetCompleted.zip'  # Nome dell'archivio\n",
        "zip_path = create_zip_archive(folder_dest, zip_filename)\n"
      ],
      "metadata": {
        "id": "izlU7_5vC7VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Token di accesso Zenodo (sostituisci con il tuo)\n",
        "ACCESS_TOKEN = 'uVSb7icJqT9efPM71KYgviJ50r7eML9ynei2q7hDkedVlFrf8fBsr9lFaJ3O'\n",
        "\n",
        "# Crea una nuova deposizione\n",
        "def create_deposition(title):\n",
        "    url = 'https://zenodo.org/api/deposit/depositions'\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "\n",
        "    data = {\n",
        "        'metadata': {\n",
        "            'title': title,\n",
        "            'upload_type': 'dataset',\n",
        "            'description': 'Dataset WSI project MLiA',\n",
        "            'creators': [{'name': 'Raf-Tony-Luca'}]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    r = requests.post(url, params=params, data=json.dumps(data), headers=headers)\n",
        "    return r.json()\n",
        "\n",
        "# Carica il file\n",
        "def upload_file(deposition_id, file_path):\n",
        "    # Get bucket URL\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.get(url, params=params)\n",
        "    bucket_url = r.json()[\"links\"][\"bucket\"]\n",
        "\n",
        "    # Upload file\n",
        "    filename = os.path.basename(file_path)\n",
        "    with open(file_path, \"rb\") as fp:\n",
        "        r = requests.put(f\"{bucket_url}/{filename}\",\n",
        "                        data=fp,\n",
        "                        params=params)\n",
        "    return r.json()\n",
        "\n",
        "# Pubblica il dataset\n",
        "def publish_deposition(deposition_id):\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}/actions/publish'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.post(url, params=params)\n",
        "    return r.json()\n"
      ],
      "metadata": {
        "id": "ohZwuGFnC9E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esegui l'upload\n",
        "print(\"Creando deposizione...\")\n",
        "deposition = create_deposition(\"dataset_trident_\"+ feature_extractor + \"_AugDiff\")\n",
        "deposition_id = deposition['id']\n",
        "\n",
        "print(f\"Caricando file... (ID: {deposition_id})\")\n",
        "upload_result = upload_file(deposition_id, zip_filename)\n",
        "\n",
        "print(\"Pubblicando dataset...\")\n",
        "publication = publish_deposition(deposition_id)\n",
        "\n",
        "print(f\"Dataset pubblicato! DOI: {publication['doi']}\")\n",
        "print(f\"URL: {publication['links']['record_html']}\")"
      ],
      "metadata": {
        "id": "Sis-m6OyC-7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}