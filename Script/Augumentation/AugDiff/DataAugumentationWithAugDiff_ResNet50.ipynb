{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import h5py"
      ],
      "metadata": {
        "id": "9HPBozsXB85r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://zenodo.org/records/15711642/files/datasetTrident_resnet.zip?download=1\"\n",
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Train.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = './'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('./Train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNrSVq29p3r",
        "outputId": "2e330afd-be6a-4a85-ae97-5e8756779bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-26 13:45:02--  https://zenodo.org/records/15711642/files/datasetTrident_resnet.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.45.92, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1044466718 (996M) [application/octet-stream]\n",
            "Saving to: ‚Äò/content/Train.zip‚Äô\n",
            "\n",
            "/content/Train.zip  100%[===================>] 996.08M  19.2MB/s    in 50s     \n",
            "\n",
            "2025-06-26 13:45:52 (20.0 MB/s) - ‚Äò/content/Train.zip‚Äô saved [1044466718/1044466718]\n",
            "\n",
            "Extracted files: ['.config', 'trident_processed_resnet', '.ipynb_checkpoints', 'results_features', 'Train.zip', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.py\n",
        "import os, h5py, torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WSIFeatureBag(Dataset):\n",
        "    def __init__(self, h5_path, cls_label):\n",
        "        self.path = h5_path\n",
        "        self.label = torch.tensor(cls_label, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):          # un bag = una slide\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.path, 'r') as f:\n",
        "            feats = torch.from_numpy(f['features'][:]).float()  # (N,1024)\n",
        "        return feats, self.label\n"
      ],
      "metadata": {
        "id": "4XujdcPFyDTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models/vae.py\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class FeatureVAE(nn.Module):\n",
        "    def __init__(self, in_dim=1024, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.mu  = nn.Linear(256, latent_dim)\n",
        "        self.logv= nn.Linear(256, latent_dim)\n",
        "\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, 512)\n",
        "        self.fc5 = nn.Linear(512, in_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
        "        return self.mu(h), self.logv(h)\n",
        "\n",
        "    def reparameterize(self, mu, logv):\n",
        "        std = torch.exp(0.5*logv)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(F.relu(self.fc3(z))))\n",
        "        return self.fc5(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logv = self.encode(x)\n",
        "        z  = self.reparameterize(mu, logv)\n",
        "        xr = self.decode(z)\n",
        "        return xr, mu, logv\n"
      ],
      "metadata": {
        "id": "QqkskDJeyfy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build_dataset.py\n",
        "from glob import glob\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "rootB = \"./trident_processed_resnet/B/20x_256px_0px_overlap/features_resnet50/\"\n",
        "filesB   = sorted(glob(f\"{rootB}/*.h5\"))\n",
        "labelB = np.zeros(len(filesB), dtype=int)\n",
        "rootE = \"./trident_processed_resnet/E/20x_256px_0px_overlap/features_resnet50/\"\n",
        "filesE   = sorted(glob(f\"{rootE}/*.h5\"))\n",
        "labelE = np.ones(len(filesE), dtype=int)\n",
        "rootS = \"./trident_processed_resnet/S/20x_256px_0px_overlap/features_resnet50/\"\n",
        "filesS   = sorted(glob(f\"{rootS}/*.h5\"))\n",
        "labelS = np.ones(len(filesS), dtype=int)*2\n",
        "\n",
        "files = filesB + filesE + filesS\n",
        "labels = labesB + labelsE + labelsS\n",
        "\n",
        "bags = [WSIFeatureBag(p,l) for p,l in zip(files, labels)]\n",
        "dataset = ConcatDataset(bags)       # 22 bag\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "yZ5knes8yP2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = FeatureVAE().cuda()\n",
        "opt = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
        "for epoch in range(30):\n",
        "    for feats,_ in loader:                 # loader patch-wise\n",
        "        feats = feats.cuda()\n",
        "        xr, mu, logv = vae(feats)\n",
        "        mse = F.mse_loss(xr, feats)\n",
        "        kld = -0.5*torch.mean(1+logv-mu.pow(2)-logv.exp())\n",
        "        loss = mse + 1e-3*kld\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n"
      ],
      "metadata": {
        "id": "vlKDejG06nAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models/latent_unet.py\n",
        "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
        "\n",
        "latent_dim = 128\n",
        "unet = Unet(\n",
        "    dim=64, dim_mults=(1,2,4,8), channels=latent_dim\n",
        ")\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model=unet, image_size=1,   # vettori 1√ó1√óD trattati come ‚Äúimmagini‚Äù\n",
        "    timesteps=1000,             # T\n",
        "    loss_type='l2'\n",
        ")\n"
      ],
      "metadata": {
        "id": "z6zdhU9e7c-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    for feats,_ in loader:\n",
        "        z = vae.encode(feats.cuda())[0]\n",
        "        loss = diffusion(z)\n",
        "        loss.backward(); opt.step(); opt.zero_grad()\n"
      ],
      "metadata": {
        "id": "5yT4-JPa7htg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_features(feats, K_ratio=0.2):\n",
        "    with torch.no_grad():\n",
        "        z0 = vae.encode(feats)[0]                  # (N,128)\n",
        "        t  = int(K_ratio*diffusion.num_timesteps)\n",
        "        noise = torch.randn_like(z0)\n",
        "        zt = diffusion.q_sample(x_start=z0, t=torch.full((len(z0),),t).cuda(), noise=noise)\n",
        "        z_hat = diffusion.p_sample_loop(z_t=zt, t_start=t)\n",
        "        feats_aug = vae.decode(z_hat)\n",
        "    return torch.cat([feats_aug],0)\n"
      ],
      "metadata": {
        "id": "7_--E2GF7jum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models/abmil.py\n",
        "class ABMIL(nn.Module):\n",
        "    def __init__(self, in_dim=1024, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.Tanh()\n",
        "        )\n",
        "        self.att  = nn.Sequential(\n",
        "            nn.Linear(256,128), nn.Tanh(),\n",
        "            nn.Linear(128,1)\n",
        "        )\n",
        "        self.cls  = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, bag):\n",
        "        H = self.embed(bag)              # (n_inst,256)\n",
        "        A = torch.softmax(self.att(H),0) # (n_inst,1)\n",
        "        M = torch.sum(A*H, 0, keepdim=True)\n",
        "        return self.cls(M)               # (1,3)\n"
      ],
      "metadata": {
        "id": "TGroDuPv7qnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mil = ABMIL().cuda()\n",
        "opt = torch.optim.Adam(mil.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "Kh_zRQZQ-TbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "for bag, y in bag_loader:          # bag_loader restituisce un bag per step\n",
        "    feats = bag[0]                 # (N,1024)\n",
        "    feats = augment_features(feats.cuda())   # concat original+aug\n",
        "    y = y.cuda()\n",
        "\n",
        "    logits = mil(feats)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n"
      ],
      "metadata": {
        "id": "Wj98As2q7s0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasetResnetExtrapolation"
      ],
      "metadata": {
        "id": "2ka9J_1O_Um8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_f = []\n",
        "E_f = []\n",
        "S_f = []\n",
        "index_p = 0\n",
        "bag_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "for bag, y in bag_loader:          # bag_loader restituisce un bag per step\n",
        "    feats = bag[0]                 # (N,1024)\n",
        "    aug_feats = augment_features(feats.cuda())   # concat original+aug\n",
        "    if y == 0:\n",
        "        B_f.append(feats)\n",
        "        B_f.append(aug_feats)\n",
        "    elif y == 1:\n",
        "        E_f.append(feats)\n",
        "        E_f.append(aug_feats)\n",
        "    else:\n",
        "        S_f.append(feats)\n",
        "        S_f.append(aug_feats)\n"
      ],
      "metadata": {
        "id": "Qi-8u5JN-t7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = filesS\n",
        "labels = labelsS\n",
        "\n",
        "bags = [WSIFeatureBag(p,l) for p,l in zip(files, labels)]\n",
        "dataset = ConcatDataset(bags)       # 22 bag\n",
        "bag_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "for bag, y in bag_loader:          # bag_loader restituisce un bag per step\n",
        "    feats = bag[0]                 # (N,1024)\n",
        "    aug_feats = augment_features(feats.cuda())   # concat original+aug\n",
        "    if y == 0:\n",
        "        B_f.append(feats)\n",
        "        B_f.append(aug_feats)\n",
        "    elif y == 1:\n",
        "        E_f.append(feats)\n",
        "        E_f.append(aug_feats)\n",
        "    else:\n",
        "        S_f.append(feats)\n",
        "        S_f.append(aug_feats)"
      ],
      "metadata": {
        "id": "uQyxtfumBjY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_csv = []\n",
        "j = 0\n",
        "\n",
        "folder_dest = \"./datasetResnetExtrapolation/\"\n",
        "\n",
        "for i in range(len(B_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(B_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'B'])\n",
        "for i in range(len(S_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(S_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'S'])\n",
        "for i in range(len(E_f)):\n",
        "    v = \"M-\" + j\n",
        "    j += 1\n",
        "    torch.save(E_f[i], folder_dest + v + \".pt\")\n",
        "    dataset_csv.append([v,v,'E'])"
      ],
      "metadata": {
        "id": "44Jd-KzxB7La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Nome del file CSV\n",
        "file_csv = folder_dest + 'datasetComposition.csv'\n",
        "\n",
        "# Scrittura del file CSV\n",
        "with open(file_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Scrivere l'intestazione\n",
        "    writer.writerow(['case_id', 'slide_id', 'label'])\n",
        "    # Scrivere i dati\n",
        "    writer.writerows(dataset_csv)\n",
        "\n",
        "print(f\"File CSV salvato: {file_csv}\")"
      ],
      "metadata": {
        "id": "Tl8P8JkqCyAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un archivio ZIP della cartella\n",
        "def create_zip_archive(folder_path, zip_name=None):\n",
        "    \"\"\"\n",
        "    Crea un archivio ZIP di una cartella mantenendo la struttura\n",
        "    \"\"\"\n",
        "    if zip_name is None:\n",
        "        zip_name = f\"{os.path.basename(folder_path)}.zip\"\n",
        "\n",
        "    print(f\"üóúÔ∏è Creando archivio ZIP: {zip_name}\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Mantiene la struttura delle cartelle nell'archivio\n",
        "                arc_name = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
        "                zipf.write(file_path, arc_name)\n",
        "                print(f\"  üìÅ Aggiunto: {arc_name}\")\n",
        "\n",
        "    # Mostra dimensione dell'archivio\n",
        "    zip_size = os.path.getsize(zip_name)\n",
        "    print(f\"‚úÖ Archivio creato: {zip_name} ({zip_size/1024/1024:.2f} MB)\")\n",
        "    return zip_name"
      ],
      "metadata": {
        "id": "zZhX2Y68C54e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso della cartella da caricare\n",
        "zip_filename = 'datasetCompleted.zip'  # Nome dell'archivio\n",
        "zip_path = create_zip_archive(folder_dest, zip_filename)\n"
      ],
      "metadata": {
        "id": "izlU7_5vC7VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Token di accesso Zenodo (sostituisci con il tuo)\n",
        "ACCESS_TOKEN = 'uVSb7icJqT9efPM71KYgviJ50r7eML9ynei2q7hDkedVlFrf8fBsr9lFaJ3O'\n",
        "\n",
        "# Crea una nuova deposizione\n",
        "def create_deposition(title):\n",
        "    url = 'https://zenodo.org/api/deposit/depositions'\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "\n",
        "    data = {\n",
        "        'metadata': {\n",
        "            'title': title,\n",
        "            'upload_type': 'dataset',\n",
        "            'description': 'Dataset WSI project MLiA',\n",
        "            'creators': [{'name': 'Raf-Tony-Luca'}]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    r = requests.post(url, params=params, data=json.dumps(data), headers=headers)\n",
        "    return r.json()\n",
        "\n",
        "# Carica il file\n",
        "def upload_file(deposition_id, file_path):\n",
        "    # Get bucket URL\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.get(url, params=params)\n",
        "    bucket_url = r.json()[\"links\"][\"bucket\"]\n",
        "\n",
        "    # Upload file\n",
        "    filename = os.path.basename(file_path)\n",
        "    with open(file_path, \"rb\") as fp:\n",
        "        r = requests.put(f\"{bucket_url}/{filename}\",\n",
        "                        data=fp,\n",
        "                        params=params)\n",
        "    return r.json()\n",
        "\n",
        "# Pubblica il dataset\n",
        "def publish_deposition(deposition_id):\n",
        "    url = f'https://zenodo.org/api/deposit/depositions/{deposition_id}/actions/publish'\n",
        "    params = {'access_token': ACCESS_TOKEN}\n",
        "    r = requests.post(url, params=params)\n",
        "    return r.json()\n"
      ],
      "metadata": {
        "id": "ohZwuGFnC9E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esegui l'upload\n",
        "print(\"Creando deposizione...\")\n",
        "deposition = create_deposition(\"dataset_trident_resnet50_AugDiff\")\n",
        "deposition_id = deposition['id']\n",
        "\n",
        "print(f\"Caricando file... (ID: {deposition_id})\")\n",
        "upload_result = upload_file(deposition_id, zip_filename)\n",
        "\n",
        "print(\"Pubblicando dataset...\")\n",
        "publication = publish_deposition(deposition_id)\n",
        "\n",
        "print(f\"Dataset pubblicato! DOI: {publication['doi']}\")\n",
        "print(f\"URL: {publication['links']['record_html']}\")"
      ],
      "metadata": {
        "id": "Sis-m6OyC-7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}